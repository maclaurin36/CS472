{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVL7_bgmIAPR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Backpropagation Lab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6ZbYjZZZ_yLV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCcEPx5VIORj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. (40%) Correctly implement and submit your own code for the backpropagation algorithm. \n",
    "\n",
    "## Code requirements \n",
    "- Ability to create a network structure with at least one hidden layer and an arbitrary number of nodes.\n",
    "- Random weight initialization with small random weights with mean of 0 and a variance of 1.\n",
    "- Use Stochastic/On-line training updates: Iterate and update weights after each training instance (i.e. do not attempt batch updates)\n",
    "- Implement a validation set based stopping criterion.\n",
    "- Shuffle training set at each epoch.\n",
    "- Option to include a momentum term\n",
    "\n",
    "You may use your own random train/test split or use the scikit-learn version if you want.\n",
    "\n",
    "Use your Backpropagation algorithm to solve the Debug data. We provide you with several parameters, and you should be able to replicate our results every time. When you are confident it is correct, run your script on the Evaluation data with the same parameters, and print your final weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_a2KSZ_7AN0G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from scipy.io import arff\n",
    "import urllib\n",
    "import math\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class MLP(BaseEstimator,ClassifierMixin):\n",
    "\n",
    "    def __init__(self,lr=.1, momentum=0, shuffle=True,hidden_layer_widths=None):\n",
    "        \"\"\" Initialize class with chosen hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            lr (float): A learning rate / step size.\n",
    "            shuffle(boolean): Whether to shuffle the training data each epoch. DO NOT SHUFFLE for evaluation / debug datasets.\n",
    "            momentum(float): The momentum coefficent \n",
    "        Optional Args (Args we think will make your life easier):\n",
    "            hidden_layer_widths (list(int)): A list of integers which defines the width of each hidden layer if hidden layer is none do twice as many hidden nodes as input nodes. (and then one more for the bias node)\n",
    "            For example: input width 1, then hidden layer will be 3 nodes\n",
    "        Example:\n",
    "            mlp = MLP(lr=.2,momentum=.5,shuffle=False,hidden_layer_widths = [3,3]),  <--- this will create a model with two hidden layers, both 3 nodes wide\n",
    "        \"\"\"\n",
    "        self.hidden_layer_widths = hidden_layer_widths\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.shuffle = shuffle\n",
    "        self.weights = []\n",
    "        self.prev_change_in_weights = []\n",
    "        self.output_converter = np.vectorize(_apply_output_single_node)\n",
    "        self.f_prime_converter = np.vectorize(_get_f_prime_of_output)\n",
    "\n",
    "        self.runs_left = 0\n",
    "        self.is_deterministic = False\n",
    "\n",
    "        self.best_val_score = 0\n",
    "        self.num_epochs_wo_sig_imp = 0\n",
    "\n",
    "    def fit(self, X, y, gaussian=True, initial_weights=None, deterministic=0, val_perc=.15):\n",
    "        \"\"\" Fit the data; run the algorithm and adjust the weights to find a good solution\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "            y (array-like): A 2D numpy array with the training targets\n",
    "        Optional Args (Args we think will make your life easier):\n",
    "            initial_weights (array-like): allows the user to provide initial weights\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "\n",
    "        \"\"\"\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        X_validate = []\n",
    "        y_validate = []\n",
    "        if deterministic != 0:\n",
    "            self.is_deterministic = True\n",
    "            self.runs_left = deterministic\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "        else:\n",
    "            X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=val_perc, random_state=42)\n",
    "\n",
    "        self.weights = self.initialize_weights(X_train, y_train, gaussian) if not initial_weights else initial_weights\n",
    "        self._init_prev_change_in_weights()\n",
    "        X_with_bias = self._add_bias_to_matrix(X_train)\n",
    "\n",
    "        while not self.stopping_criteria_met(X_validate, y_validate):\n",
    "            if self.shuffle:\n",
    "                X_train, y_train = self._shuffle_data(X_train, y_train)\n",
    "            # Go through all data once\n",
    "            for data_row in range(0, len(X_with_bias)):\n",
    "                # Get the current row of data\n",
    "                cur_inputs = np.array(X_with_bias[data_row])\n",
    "                cur_targets = np.array(y_train[data_row])\n",
    "\n",
    "                # Get the outputs (Z) for the whole net\n",
    "                iteration_outputs = self._get_iteration_outputs(cur_inputs)\n",
    "\n",
    "                # Get the errors (delta) for the whole net (output and hidden have different formulas)\n",
    "                output_errors = self._get_all_output_nodes_errors(cur_targets, iteration_outputs[len(iteration_outputs)-1])\n",
    "                hidden_node_errors = self._get_all_hidden_node_errors(iteration_outputs, output_errors)\n",
    "\n",
    "                # Update the weights\n",
    "                all_errors = hidden_node_errors.copy()\n",
    "                all_errors.append(output_errors)\n",
    "                inputs_wo_bias = np.array(X_train[data_row])\n",
    "                all_but_final_output = [inputs_wo_bias]\n",
    "                all_but_final_output.extend(iteration_outputs[:-1])\n",
    "                self._update_all_weights(all_errors, all_but_final_output)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def stopping_criteria_met(self, X_validate, y_validate):\n",
    "        if self.is_deterministic:\n",
    "            self.runs_left -= 1\n",
    "            if self.runs_left < 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            validation_score = self.score(X_validate, y_validate)\n",
    "            if validation_score > self.best_val_score:\n",
    "                self.best_val_score = validation_score\n",
    "                self.num_epochs_wo_sig_imp = 0\n",
    "            else:\n",
    "                self.num_epochs_wo_sig_imp += 1\n",
    "\n",
    "            if self.num_epochs_wo_sig_imp > 20:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict all classes for a dataset X\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data, excluding targets\n",
    "        Returns:\n",
    "            array, shape (n_samples,)\n",
    "                Predicted target values per element in X.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def initialize_weights(self, X, y, gaussian):\n",
    "        \"\"\" Initialize weights for perceptron. Don't forget the bias!\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        _, num_features = X.shape\n",
    "        _, num_outputs = y.shape\n",
    "        if self.hidden_layer_widths == None:\n",
    "            self.hidden_layer_widths = [num_features * 2]\n",
    "        weights = []\n",
    "        input_layer_weights = self._initialize_single_array(num_features + 1, self.hidden_layer_widths[0], gaussian)\n",
    "\n",
    "        weights.append(input_layer_weights)\n",
    "\n",
    "        for i in range(0, len(self.hidden_layer_widths)-1):\n",
    "            cur_layer_weights = self._initialize_single_array(self.hidden_layer_widths[i]+1,self.hidden_layer_widths[i+1], gaussian)\n",
    "            weights.append(cur_layer_weights)\n",
    "\n",
    "        output_layer_weights = self._initialize_single_array(self.hidden_layer_widths[len(self.hidden_layer_widths)-1] + 1, num_outputs, gaussian)\n",
    "        weights.append(output_layer_weights)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return accuracy of model on a given dataset. Must implement own score function.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with data, excluding targets\n",
    "            y (array-like): A 2D numpy array with targets\n",
    "\n",
    "        Returns:\n",
    "            score : float\n",
    "                Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def _shuffle_data(self, X, y):\n",
    "        \"\"\" Shuffle the data! This _ prefix suggests that this method should only be called internally.\n",
    "            It might be easier to concatenate X & y and shuffle a single 2D array, rather than\n",
    "             shuffling X and y exactly the same way, independently.\n",
    "        \"\"\"\n",
    "        shuffledX, shuffledY = shuffle(X, y)\n",
    "        return shuffledX, shuffledY\n",
    "\n",
    "    ### Not required by sk-learn but required by us for grading. Returns the weights.\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def _initialize_single_array(self, rows, cols, gaussian):\n",
    "        if gaussian:\n",
    "            return np.random.normal(loc=0.0, scale=0.001, size=(rows,cols))\n",
    "        else:\n",
    "            return np.zeros((rows, cols))\n",
    "\n",
    "    def _add_bias_to_matrix(self, npArray):\n",
    "        _, num_cols = npArray.shape\n",
    "        return np.insert(npArray, num_cols, 1, axis=1)\n",
    "\n",
    "    def _get_iteration_outputs(self, inputs_with_bias):\n",
    "        cur_inputs = inputs_with_bias\n",
    "        outputs = []\n",
    "        for i in range(0,len(self.weights)):\n",
    "            cur_layer_net = cur_inputs.dot(self.weights[i])\n",
    "            cur_layer_out = self._apply_output_function(cur_layer_net)\n",
    "            outputs.append(cur_layer_out)\n",
    "            cur_inputs = np.insert(cur_layer_out, len(cur_layer_out), 1)\n",
    "        return outputs\n",
    "\n",
    "    def _apply_output_function(self, net_array):\n",
    "        return self.output_converter(net_array)\n",
    "\n",
    "    def _get_all_output_nodes_errors(self, cur_targets, final_outputs):\n",
    "        num_outputs = final_outputs.shape[0]\n",
    "        output_errors = []\n",
    "        for i in range(0, num_outputs):\n",
    "            output_errors.append(self._get_output_node_error(cur_targets[i], final_outputs[i]))\n",
    "        return np.array(output_errors)\n",
    "\n",
    "    def _get_output_node_error(self, target, output):\n",
    "        return (target - output) * (output) * (1 - output)\n",
    "\n",
    "    def _get_all_hidden_node_errors(self, all_outputs, output_errors):\n",
    "        k_errors = output_errors\n",
    "        hidden_node_errors = []\n",
    "        for i in range(len(all_outputs)-2, -1, -1):\n",
    "            j_outputs = all_outputs[i]\n",
    "            j_k_weights = self.weights[i + 1]\n",
    "            cur_layer_errors = self._get_hidden_node_layer_error(j_outputs, k_errors, j_k_weights)\n",
    "            hidden_node_errors.append(cur_layer_errors)\n",
    "            k_errors = cur_layer_errors\n",
    "        return hidden_node_errors\n",
    "\n",
    "    def _get_hidden_node_layer_error(self, j_outputs, k_errors, j_k_weights):\n",
    "        direct_term = np.dot(j_k_weights, k_errors)\n",
    "        j_f_primes = self.f_prime_converter(j_outputs)\n",
    "        cur_layer_errors = []\n",
    "        for i in range(0, len(j_f_primes)):\n",
    "            cur_layer_errors.append(j_f_primes[i] * direct_term[i])\n",
    "        return np.array(cur_layer_errors)\n",
    "\n",
    "    def _update_all_weights(self, all_node_errors, inh_wob_node_outputs):\n",
    "        for layer_index in range(0, len(all_node_errors)):\n",
    "            i_j_weights = self.weights[layer_index]\n",
    "            j_errors = all_node_errors[layer_index]\n",
    "            i_outputs = np.insert(inh_wob_node_outputs[layer_index], len(inh_wob_node_outputs[layer_index]), 1)\n",
    "            prev_i_j_change_weights = self.prev_change_in_weights[layer_index]\n",
    "            self._update_layer_weights(i_j_weights, j_errors, i_outputs, prev_i_j_change_weights)\n",
    "\n",
    "    def _update_layer_weights(self, layer_weights, j_errors, i_outputs, prev_i_j_change_weights):\n",
    "        for i in range(0, len(i_outputs)):\n",
    "            for j in range(0, len(j_errors)):\n",
    "                new_change_weight = self.lr * j_errors[j] * i_outputs[i] + self.momentum * prev_i_j_change_weights[i][j]\n",
    "                layer_weights[i][j] += new_change_weight\n",
    "                prev_i_j_change_weights[i][j] = new_change_weight\n",
    "\n",
    "\n",
    "    def _init_prev_change_in_weights(self):\n",
    "        for i in range(0, len(self.weights)):\n",
    "            num_rows, num_cols = self.weights[i].shape\n",
    "            self.prev_change_in_weights.append(np.zeros((num_rows, num_cols)))\n",
    "\n",
    "def _get_f_prime_of_output(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "def _apply_output_single_node(net):\n",
    "    output = 1.0 / (1.0 + math.exp(-net))\n",
    "    return output\n",
    "\n",
    "def load_data(url: str):\n",
    "    ftp_stream = urllib.request.urlopen(url)\n",
    "    data, meta = arff.loadarff(io.StringIO(ftp_stream.read().decode('utf-8')))\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    return data_frame\n",
    "\n",
    "def remove_bytes(num):\n",
    "    if type(num) == bytes:\n",
    "        return ord(num) - 48\n",
    "    return num\n",
    "\n",
    "byteRemoverFunction = np.vectorize(remove_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KibCIXIThpbE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1 \n",
    "\n",
    "Debug your model using the following parameters:\n",
    "\n",
    "Learning Rate = 0.1\\\n",
    "Momentum = 0.5\\\n",
    "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
    "Shuffle = False\\\n",
    "Validation size = 0\\\n",
    "Initial Weights = All zeros\\\n",
    "Hidden Layer Widths = [4]\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1.1 Debug \n",
    "\n",
    "Debug your model by running it on the [Debug Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/linsep2nonorigin.arff)\n",
    "\n",
    "\n",
    "Expected Results for Binary Classification (i.e. 1 output node): [debug_bp_0.csv](https://github.com/cs472ta/CS472/blob/master/debug_solutions/debug_bp_0.csv) \n",
    "\n",
    "$$ \\text{Layer 1} = \\begin{bmatrix} -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 & -8.81779797\\text{e}-05 \\\\ 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 & 7.82757731\\text{e}-04 \\\\ -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 & -3.94353645\\text{e}-03 \\end{bmatrix}$$\n",
    "                                             \n",
    "$$ \\text{Layer 2} = \\begin{bmatrix} -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.01060888 \\\\ -0.02145495 \\end{bmatrix}$$\n",
    "\n",
    "(The weights do not need to be in this order or shape.)\n",
    "\n",
    "Expected Results for One Hot Vector Classification (i.e. 2 output nodes): [debug_bp_2outs.csv](https://github.com/cs472ta/CS472/blob/master/debug_solutions/debug_bp_2outs.csv) \n",
    "\n",
    "$$ \\text{Layer 1} = \\begin{bmatrix} -0.00018149 & -0.00018149 & -0.00018149 & -0.00018149 \\\\ 0.00157468 & 0.00157468 & 0.00157468 & 0.00157468 \\\\ -0.00788218 & -0.00788218 & -0.00788218 & -0.00788218 \\end{bmatrix}$$\n",
    "                          \n",
    "$$ \\text{Layer 2} = \\begin{bmatrix} 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.01050642 & -0.01050642 \\\\ 0.02148778 & -0.02148778 \\end{bmatrix}$$\n",
    "\n",
    "(The weights do not need to be in this order or shape.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KgAyy82gixIF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Encoding Weights\n",
      "Layer 1\n",
      "[[-8.81779797e-05 -8.81779797e-05 -8.81779797e-05 -8.81779797e-05]\n",
      " [ 7.82757731e-04  7.82757731e-04  7.82757731e-04  7.82757731e-04]\n",
      " [-3.94353645e-03 -3.94353645e-03 -3.94353645e-03 -3.94353645e-03]]\n",
      "Layer 2\n",
      "[[-0.01060888]\n",
      " [-0.01060888]\n",
      " [-0.01060888]\n",
      " [-0.01060888]\n",
      " [-0.02145495]]\n",
      "\n",
      "One-Hot Encoding Weights\n",
      "Layer 1\n",
      "[[-0.00018149 -0.00018149 -0.00018149 -0.00018149]\n",
      " [ 0.00157468  0.00157468  0.00157468  0.00157468]\n",
      " [-0.00788218 -0.00788218 -0.00788218 -0.00788218]]\n",
      "Layer 2\n",
      "[[ 0.01050642 -0.01050642]\n",
      " [ 0.01050642 -0.01050642]\n",
      " [ 0.01050642 -0.01050642]\n",
      " [ 0.01050642 -0.01050642]\n",
      " [ 0.02148778 -0.02148778]]\n"
     ]
    }
   ],
   "source": [
    "# Load debug data\n",
    "debug_data = load_data(r'https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/linsep2nonorigin.arff')\n",
    "debug_data = byteRemoverFunction(debug_data)\n",
    "X = debug_data[:,:-1]\n",
    "y = debug_data[:,-1, None]\n",
    "\n",
    "# Train on binary debug data\n",
    "debug_mlp_binary = MLP(momentum=0.5, shuffle=False,hidden_layer_widths=[4])\n",
    "debug_mlp_binary.fit(X, y, gaussian=False,deterministic=10)\n",
    "\n",
    "# Print weights for binary\n",
    "debug_binary_weights = debug_mlp_binary.get_weights()\n",
    "print('Binary Encoding Weights')\n",
    "for i in range(0,len(debug_binary_weights)):\n",
    "    print(f'Layer {i+1}')\n",
    "    print(debug_binary_weights[i])\n",
    "\n",
    "# Train on one hot debug data\n",
    "debug_enc = OneHotEncoder()\n",
    "y_hot = debug_enc.fit_transform(y).toarray()\n",
    "debug_mlp_hot = MLP(momentum=0.5, shuffle=False,hidden_layer_widths=[4])\n",
    "debug_mlp_hot.fit(X, y_hot, gaussian=False,deterministic=10)\n",
    "\n",
    "# Print weights for one-hot\n",
    "debug_hot_weights = debug_mlp_hot.get_weights()\n",
    "print()\n",
    "print('One-Hot Encoding Weights')\n",
    "for i in range(0,len(debug_hot_weights)):\n",
    "    print(f'Layer {i+1}')\n",
    "    print(debug_hot_weights[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY3VNB1ui03N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1.2 Evaluation\n",
    "\n",
    "Evaluate your model using the following parameters:\n",
    "\n",
    "Learning Rate = 0.1\\\n",
    "Momentum = 0.5\\\n",
    "Deterministic = 10 [This means run it 10 epochs and should be the same everytime you run it]\\\n",
    "Shuffle = False\\\n",
    "Validation size = 0\\\n",
    "Initial Weights = All zeros\\\n",
    "Hidden Layer Widths = [4]\n",
    "\n",
    "We will evaluate your model based on printed weights after training on the [Evaluation Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/data_banknote_authentication.arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "2yAxA78QjDh2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "\n",
    "# Train on evaluation data\n",
    "\n",
    "# Print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vWiTdlbR2Xh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. (10%) Backpropagation on the Iris Classification problem.\n",
    "\n",
    "Load the Iris Dataset [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
    "\n",
    "Parameters:\n",
    "- One layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
    "- Use a 80/20 split of the data for the training/test set.\n",
    "- Use a learning rate of 0.1\n",
    "- Use a validation set (15% of the training set) taken from the training set for your stopping criteria\n",
    "- Create one graph with MSE (mean squared error) over epochs from the training set and validation set\n",
    "- Create one graph with classification accuracy (% classified correctly) over epochs from the training set and validation set\n",
    "- Print out your test set accuracy\n",
    "\n",
    "The results for the different measurables should be shown with a different color, line type, etc. Typical backpropagation accuracies for the Iris data set are 85-95%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SSoasDQSKXb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iris Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. (10%) Working with the Vowel Dataset - Learning Rate\n",
    "\n",
    "Load the Vowel Dataset [Vowel Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/vowel.arff)\n",
    "\n",
    "- Use one layer of hidden nodes with the number of hidden nodes being twice the number of inputs.\n",
    "- Use random 80/20 splits of the data for the training/test set.\n",
    "- Use a validation set (15% of the training set) taken from the training set for your stopping criteria\n",
    "- Try some different learning rates (LR). Note that each LR will probably require a different number of epochs to learn. \n",
    "\n",
    "- For each LR you test, plot their validation's set MSE over Epochs on the same graph. Graph 4-5 different LRs and make them different enough to see a difference between them.\n",
    "\n",
    "In general, whenever you are testing a parameter such as LR, # of hidden nodes, etc., test values until no more improvement is found. For example, if 20 hidden nodes did better than 10, you would not stop at 20, but would try 40, etc., until you no longer get improvement.\n",
    "\n",
    "If you would like you may average the results of multiple initial conditions (e.g. 3) per LR, and that obviously would give more accurate results.\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/cs472ta/CS472/master/images/backpropagation/backprop_val_set_MSE_vs_epochs.png width=500 height=500  align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KBGUn43ASiXW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train on each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOteTlV6S0bq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1 (5%) Working with the Vowel Dataset - Intuition\n",
    "- Discuss the effect of varying learning rates. \n",
    "- Discuss why the vowel data set might be more difficult than Iris\n",
    "    - Report both datasets' baseline accuracies and best **test** set accuracies. \n",
    "- Consider which of the vowel dataset's given input features you should actually use (Train/test, speaker, gender, ect) and discuss why you chose the ones you did.\n",
    "\n",
    "Typical backpropagation accuracies for the Vowel data set are above 75%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Discuss Intuition here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 (10%) Working with the Vowel Dataset - Hidden Layer Nodes\n",
    "\n",
    "Using the best LR you discovered, experiment with different numbers of hidden nodes.\n",
    "\n",
    "- Start with 1 hidden node, then 2, and then double them for each test until you get no more improvement in accuracy. \n",
    "- For each number of hidden nodes find the best validation set solution (in terms of validation set MSE).  \n",
    "- Create one graph with MSE for the training set and validation set on the y-axis and # of hidden nodes on the x-axis.\n",
    "- Report the final test set accuracy for every # of hidden nodes you experimented on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Discuss Hidden Layer Nodes here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIRG42TgSR4x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 (10%) Working with the Vowel Dataset - Momentum\n",
    "\n",
    "Try some different momentum terms using the best number of hidden nodes and LR from your earlier experiments.\n",
    "\n",
    "- Create a graph similar to step 3.2, but with momentum on the x-axis and number of epochs until validation set convergence on the y-axis.\n",
    "- For each momentum term, print the test set accuracy. \n",
    "- You are trying to see how much momentum speeds up learning and how it affects accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmq9GSbJS8k2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Discuss Momentum here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.1 (10%) Use the scikit-learn (SK) version of the MLP classifier on the Iris and Vowel data sets.  \n",
    "\n",
    "You do not need to go through all the steps above, nor graph results. Compare results (accuracy and learning speed) between your version and theirs for some selection of hyper-parameters. Try different hyper-parameters and comment on their effect.\n",
    "\n",
    "At a minimum, try\n",
    "\n",
    "- number of hidden nodes and layers\n",
    "- different activation functions\n",
    "- learning rate\n",
    "- regularization and parameters\n",
    "- momentum (and try nesterov)\n",
    "- early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFQv70W2VyqJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load sklearn perceptron\n",
    "\n",
    "# Train on voting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Record impressions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2 (5%) Using the Iris Dataset automatically adjust hyper-parameters using your choice of grid/random search\n",
    "- Use a grid or random search approach across a reasonable subset of hyper-parameters from the above \n",
    "- Report your best accuracy and hyper-parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OFQv70W2VyqJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load sklearn perceptron\n",
    "\n",
    "# Train on voting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlK-kijk8Mg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. (Optional 5% Extra credit) For the vowel data set, use the other hyper-parameter approach that you did not use in part 4.2 to find LR, # of hidden nodes, and momentum.  \n",
    "\n",
    "- Compare and discuss the values found with the ones you found in part 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqSFAXwlk3Ms",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Discuss findings here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab 1 - perceptron",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}